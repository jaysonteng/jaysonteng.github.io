<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Title</title>
    <link rel="icon" href="./images/k4.ico" type="image/x-icon">
    <link rel="shortcut icon" href="./images/k4.ico" type="image/x-icon">
</head>
<body>
    <h1 >梯度下降算法原理介绍</h1>
    <h3>梯度下降法</h3>
    <p>1、梯度：</p>
    <p>在微积分里面，对多元函数参数求偏导数，把求的各参数的偏导数以向量的形式写出来，就是梯度。</p>
    <p>梯度向量从几何意义上讲，就是函数变化增加最快的地方，沿着梯度向量的方向更容易找到函数的最大值，沿着向量相反的方向，梯度减小最快，更容易找到函数最小值。</p>
    <p>2、梯度下降与梯度上升可以互相转化。求损失函数f(θ)的最小值，用梯度下降法迭代，亦可反过来求损失函数 -f(θ)的最大值，用梯度上升法。</p>
    <p>3、梯度下降算法解析</p>
    <p>(1)直观解释</p>
    <p>eg.在一座大山的某一位置，要下山，走一步算一步，每走一步就计算当前位置的梯度，沿着当前梯度的负方向走一步(也就是当前最陡的位置)，然后再次计算当前位置，这样一步一步往下走，一直走到觉得已经到了山脚。有可能我们只是到了一个局部山峰底部。所以梯度下降不一定能找到全局最优解，有可能是一个局部最优解。当损失函数是凸函数的时候，梯度下降法所求的解就是全局最优解。</p>
    <p>(2)相关概念</p>
    <p>(i)步长：梯度下降迭代过程中每一步沿负方向前进的长度。</p>
    <p>(ii)特征：样本输入部分，样本(x0,y0)，其样本特征为x,输出为y。</p>
    <p>(Iii) 假设函数：在监督学习中，用假设函数拟合输入样本，记为hθ(x)。比如对于样本(xi,yi)。(i=1,2,...n),可以采用拟合函数如下：hθ(x) =θ0+θ1x。</p>
    <p>(iv)损失函数：度量拟合程度，评估模型拟合好坏。损失函数极小化，意味着拟合程度最好，对应的模型参数为最优参数。线性回归中，损失函数通常为样本输出和假设函数的差取平方。</p>
</body>
</html>